{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfd4ca50",
   "metadata": {},
   "source": [
    "# ML Algorithm Insight Series\n",
    "## Module: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550131e7",
   "metadata": {},
   "source": [
    "### 1. Introduction & Intuition\n",
    "\n",
    "Support Vector Machines (SVMs) are designed to find the optimal boundary that separates data into classes. Think of it as placing a rigid wall between two groups while maximizing the distance between the closest points of each group and the wall.\n",
    "\n",
    "SVMs are particularly effective in high-dimensional spaces and in cases where the margin between classes is clear and well-defined.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef6d141",
   "metadata": {},
   "source": [
    "### 2. How the Algorithm Works\n",
    "\n",
    "SVMs construct a hyperplane or set of hyperplanes in a high-dimensional space. These hyperplanes act as decision boundaries.\n",
    "\n",
    "The goal is to maximize the margin — the distance between the hyperplane and the nearest data points from each class, known as support vectors.\n",
    "\n",
    "The optimization problem for a linear SVM can be framed as:\n",
    "\n",
    "\\[\n",
    "\\min_\\mathbf{w}, b \\quad \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\\\\n",
    "\\text{subject to} \\quad y_i(\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1\n",
    "\\]\n",
    "\n",
    "For non-linear data, the kernel trick transforms the data into a higher-dimensional space where a linear separator can be found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc57efe",
   "metadata": {},
   "source": [
    "### 3. Data and Preparation Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1663fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
    "plt.title(\"Synthetic Classification Dataset\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f9edf9",
   "metadata": {},
   "source": [
    "Ensure data is scaled appropriately, especially for kernel SVMs. Standard scaling is essential for optimal margin computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba008f9",
   "metadata": {},
   "source": [
    "### 4. Implementation Highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8450ab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "y_pred = svc.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf9788",
   "metadata": {},
   "source": [
    "### 5. Insightful Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b5a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_decision_boundary(clf, X, y):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap='coolwarm')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolors='k', cmap='coolwarm')\n",
    "    plt.title(\"SVM Decision Boundary\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(svc, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a74fa0",
   "metadata": {},
   "source": [
    "Decision boundaries give visual intuition into model separation power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4027d",
   "metadata": {},
   "source": [
    "### 6. Algorithm Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9be0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ddbbe",
   "metadata": {},
   "source": [
    "### 7. Pros, Cons, and Techniques\n",
    "\n",
    "**Strengths:**\n",
    "- Effective in high dimensions\n",
    "- Memory efficient (uses support vectors only)\n",
    "- Flexible with kernels\n",
    "\n",
    "**Limitations:**\n",
    "- Poor performance on large datasets\n",
    "- Not suitable for noisy data\n",
    "- Choice of kernel and parameters critical\n",
    "\n",
    "**Enhancements:**\n",
    "- Use GridSearchCV for parameter tuning\n",
    "- Try different kernels (RBF, polynomial)\n",
    "- Apply dimensionality reduction before SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d19da5",
   "metadata": {},
   "source": [
    "### 8. Further Explorations\n",
    "\n",
    "- TODO: Experiment with non-linear kernels (e.g., RBF, poly)\n",
    "- TODO: Visualize support vectors and their influence\n",
    "- TODO: Compare with Logistic Regression on same dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714b67c2",
   "metadata": {},
   "source": [
    "### 9. Summary & Resources\n",
    "\n",
    "**Key Insights:**\n",
    "- SVMs are margin-based classifiers suited for clean, high-dimensional data.\n",
    "- Decision boundaries and support vectors provide interpretability.\n",
    "- Kernel tricks allow flexibility beyond linear separability.\n",
    "\n",
    "**Further Reading:**\n",
    "- “Pattern Recognition and Machine Learning” – Bishop\n",
    "- Scikit-learn Documentation: SVMs\n",
    "- Cortes & Vapnik (1995) - Support Vector Networks\n",
    "\n",
    "**Notebook Repo**: (add your GitHub link)  \n",
    "**Companion Article**: (add Medium/Substack link)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
