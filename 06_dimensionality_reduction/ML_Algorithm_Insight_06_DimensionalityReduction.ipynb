{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a50404",
   "metadata": {},
   "source": [
    "# ML Algorithm Insight Series\n",
    "## Module: Dimensionality Reduction (PCA, t-SNE, LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bda366b",
   "metadata": {},
   "source": [
    "### 1. Introduction & Intuition\n",
    "\n",
    "Dimensionality reduction techniques help simplify datasets by reducing the number of input variables while retaining meaningful structure. This enables visualization, noise reduction, and performance improvement in downstream models.\n",
    "\n",
    "Think of it as compressing a high-resolution image into a lower resolution that still preserves the essential patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47377af7",
   "metadata": {},
   "source": [
    "### 2. How the Algorithm Works\n",
    "\n",
    "**PCA (Principal Component Analysis)**:\n",
    "- Linearly transforms features into uncorrelated components\n",
    "- Maximizes variance along orthogonal axes\n",
    "- Projects data into top-k principal components\n",
    "\n",
    "PCA transformation:\n",
    "\\[\n",
    "Z = XW\n",
    "\\]\n",
    "Where \\( W \\) contains the top eigenvectors of the covariance matrix.\n",
    "\n",
    "**t-SNE (t-distributed Stochastic Neighbor Embedding)**:\n",
    "- Non-linear method\n",
    "- Preserves local structure in lower dimensions\n",
    "- Uses probabilistic distances and Kullback-Leibler divergence\n",
    "\n",
    "**LDA (Linear Discriminant Analysis)**:\n",
    "- Supervised method\n",
    "- Projects data to maximize class separability\n",
    "- Optimizes ratio of between-class to within-class scatter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e080c41",
   "metadata": {},
   "source": [
    "### 3. Data and Preparation Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd0110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "labels = iris.target_names\n",
    "\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='Set1', edgecolor='k')\n",
    "plt.title(\"Original Iris Dataset (First Two Features)\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2cc344",
   "metadata": {},
   "source": [
    "Center and scale features before applying PCA or t-SNE. LDA requires class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd35745",
   "metadata": {},
   "source": [
    "### 4. Implementation Highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40addba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=300)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_lda = lda.fit_transform(X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4144b8b6",
   "metadata": {},
   "source": [
    "### 5. Insightful Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f162a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axs[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='Set1', edgecolor='k')\n",
    "axs[0].set_title(\"PCA Projection\")\n",
    "\n",
    "axs[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='Set1', edgecolor='k')\n",
    "axs[1].set_title(\"t-SNE Projection\")\n",
    "\n",
    "axs[2].scatter(X_lda[:, 0], X_lda[:, 1], c=y, cmap='Set1', edgecolor='k')\n",
    "axs[2].set_title(\"LDA Projection\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.grid(True)\n",
    "    ax.set_xlabel(\"Component 1\")\n",
    "    ax.set_ylabel(\"Component 2\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdeb748",
   "metadata": {},
   "source": [
    "Visualizations help assess how well different methods separate or group data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e86887b",
   "metadata": {},
   "source": [
    "### 6. Algorithm Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca8814",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PCA Explained Variance Ratio:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3367c826",
   "metadata": {},
   "source": [
    "### 7. Pros, Cons, and Techniques\n",
    "\n",
    "**PCA**\n",
    "- Pros: Fast, interpretable, useful for compression\n",
    "- Cons: Linear, may not capture complex patterns\n",
    "\n",
    "**t-SNE**\n",
    "- Pros: Excellent for visualizing clusters\n",
    "- Cons: Non-deterministic, not scalable for large data\n",
    "\n",
    "**LDA**\n",
    "- Pros: Uses label info, good for class separation\n",
    "- Cons: Limited to supervised problems, assumes normality\n",
    "\n",
    "**Techniques**:\n",
    "- Use PCA for pre-processing before clustering\n",
    "- Apply t-SNE only for visualization, not modeling\n",
    "- Normalize inputs and reduce noise before reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a05a1",
   "metadata": {},
   "source": [
    "### 8. Further Explorations\n",
    "\n",
    "- TODO: Experiment with more components in PCA\n",
    "- TODO: Tune perplexity in t-SNE\n",
    "- TODO: Compare PCA with UMAP for non-linear reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc67b67",
   "metadata": {},
   "source": [
    "### 9. Summary & Resources\n",
    "\n",
    "**Key Insights:**\n",
    "- Dimensionality reduction simplifies data without losing essential structure.\n",
    "- PCA and LDA rely on linear assumptions, while t-SNE captures non-linear geometry.\n",
    "- Visualization aids in cluster detection and preprocessing analysis.\n",
    "\n",
    "**Further Reading:**\n",
    "- “Pattern Recognition and Machine Learning” – Bishop\n",
    "- Scikit-learn Documentation: PCA, t-SNE, LDA\n",
    "- van der Maaten & Hinton (2008) – t-SNE\n",
    "\n",
    "**Notebook Repo**: (add your GitHub link)  \n",
    "**Companion Article**: (add Medium/Substack link)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
