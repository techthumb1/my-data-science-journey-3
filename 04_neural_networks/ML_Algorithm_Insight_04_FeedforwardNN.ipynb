{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28e34d04",
   "metadata": {},
   "source": [
    "# ML Algorithm Insight Series\n",
    "## Module: Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec66538f",
   "metadata": {},
   "source": [
    "### 1. Introduction & Intuition\n",
    "\n",
    "Feedforward Neural Networks (FNNs), or Multi-Layer Perceptrons (MLPs), are a class of models inspired by the structure of the human brain. Information flows through the network in one direction—from input to output—making them suitable for classification and regression problems with complex, non-linear relationships.\n",
    "\n",
    "Imagine passing a signal through a series of increasingly refined filters—each layer transforms the input until it becomes something useful at the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f6dbe8",
   "metadata": {},
   "source": [
    "### 2. How the Algorithm Works\n",
    "\n",
    "An FNN consists of:\n",
    "- **Input layer**: Receives the data.\n",
    "- **Hidden layers**: Apply non-linear transformations using weights, biases, and activation functions.\n",
    "- **Output layer**: Produces final predictions.\n",
    "\n",
    "The forward pass:\n",
    "\\[\n",
    "a^{(l)} = \\sigma(W^{(l)} a^{(l-1)} + b^{(l)})\n",
    "\\]\n",
    "\n",
    "The backward pass (training phase) uses gradient descent and backpropagation to update weights:\n",
    "\n",
    "\\[\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla J(\\theta)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( \\theta \\): model parameters\n",
    "- \\( \\eta \\): learning rate\n",
    "- \\( J(\\theta) \\): loss function (e.g., MSE, cross-entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee73ac91",
   "metadata": {},
   "source": [
    "### 3. Data and Preparation Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347953b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
    "plt.title(\"Synthetic Non-Linear Dataset for FNN\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0da5cb",
   "metadata": {},
   "source": [
    "Always normalize or standardize features before inputting them into a neural network. FNNs are sensitive to input scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d16c88",
   "metadata": {},
   "source": [
    "### 4. Implementation Highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782cb58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 10), activation='relu', max_iter=1000, random_state=42)\n",
    "mlp.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d408966",
   "metadata": {},
   "source": [
    "### 5. Insightful Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fab008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap='coolwarm')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolors='k', cmap='coolwarm')\n",
    "    plt.title(\"FNN Decision Boundary\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(mlp, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05914dd3",
   "metadata": {},
   "source": [
    "The boundary adapts to complex patterns in the data, revealing the power of stacked non-linearities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4def1dc",
   "metadata": {},
   "source": [
    "### 6. Algorithm Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c400dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24586756",
   "metadata": {},
   "source": [
    "### 7. Pros, Cons, and Techniques\n",
    "\n",
    "**Strengths:**\n",
    "- Captures complex, non-linear relationships\n",
    "- Highly flexible architecture\n",
    "- Widely applicable to structured data, time-series, and images\n",
    "\n",
    "**Limitations:**\n",
    "- Requires careful tuning (e.g., learning rate, architecture)\n",
    "- Can overfit if not regularized\n",
    "- Less interpretable than simpler models\n",
    "\n",
    "**Enhancements**:\n",
    "- Use early stopping or dropout to prevent overfitting\n",
    "- Experiment with different activations (ReLU, tanh)\n",
    "- Optimize architecture via grid/random search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925d98af",
   "metadata": {},
   "source": [
    "### 8. Further Explorations\n",
    "\n",
    "- TODO: Compare different activation functions\n",
    "- TODO: Analyze effect of varying hidden layers and neurons\n",
    "- TODO: Add regularization techniques like dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358d975b",
   "metadata": {},
   "source": [
    "### 9. Summary & Resources\n",
    "\n",
    "**Key Insights:**\n",
    "- Feedforward networks learn flexible representations through layers of transformations.\n",
    "- They benefit greatly from standardized input and proper tuning.\n",
    "- While less interpretable, they unlock significant modeling power.\n",
    "\n",
    "**Further Reading:**\n",
    "- “Deep Learning” – Goodfellow, Bengio, Courville\n",
    "- Scikit-learn Documentation: MLPClassifier\n",
    "- Bishop – Neural Networks for Pattern Recognition\n",
    "\n",
    "**Notebook Repo**: (add your GitHub link)  \n",
    "**Companion Article**: (add Medium/Substack link)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
