{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0ef5980",
   "metadata": {},
   "source": [
    "# ML Algorithm Insight Series\n",
    "## Module: Recurrent Neural Networks (LSTM & GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19f9444",
   "metadata": {},
   "source": [
    "### 1. Introduction & Intuition\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are designed for sequential data—data where context and order matter. Examples include time-series forecasting, speech recognition, and natural language processing.\n",
    "\n",
    "Think of RNNs as memory-enabled networks that not only process the current input but also retain information from prior inputs in the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066d423",
   "metadata": {},
   "source": [
    "### 2. How the Algorithm Works\n",
    "\n",
    "Standard RNNs suffer from vanishing gradients over long sequences. LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) address this using gated mechanisms.\n",
    "\n",
    "LSTM Core:\n",
    "- **Forget Gate** \\( f_t \\): what to forget  \n",
    "- **Input Gate** \\( i_t \\): what new information to add  \n",
    "- **Output Gate** \\( o_t \\): what to output\n",
    "\n",
    "Core LSTM equations:\n",
    "\\[\n",
    "f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f) \\\\\n",
    "i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i) \\\\\n",
    "\\tilde{c}_t = \\tanh(W_c x_t + U_c h_{t-1} + b_c) \\\\\n",
    "c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t \\\\\n",
    "o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o) \\\\\n",
    "h_t = o_t \\odot \\tanh(c_t)\n",
    "\\]\n",
    "\n",
    "GRU is a simplified variant with fewer gates and parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a3806",
   "metadata": {},
   "source": [
    "### 3. Data and Preparation Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796eed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a noisy sine wave as synthetic time series\n",
    "x = np.linspace(0, 100, 500)\n",
    "y = np.sin(x) + 0.1 * np.random.randn(500)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Synthetic Time Series (Sine Wave + Noise)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2d4daf",
   "metadata": {},
   "source": [
    "Normalize sequences and reshape inputs to match expected format: (samples, timesteps, features)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cfdb86",
   "metadata": {},
   "source": [
    "### 4. Implementation Highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f9616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Prepare sequence dataset\n",
    "def create_sequences(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i + window_size])\n",
    "        y.append(data[i + window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "window_size = 10\n",
    "X_seq, y_seq = create_sequences(y_scaled, window_size)\n",
    "\n",
    "X_seq = X_seq.reshape(-1, window_size, 1)\n",
    "y_seq = y_seq.reshape(-1, 1)\n",
    "\n",
    "# Train-test split\n",
    "split = int(len(X_seq) * 0.8)\n",
    "X_train, X_test = X_seq[:split], X_seq[split:]\n",
    "y_train, y_test = y_seq[:split], y_seq[split:]\n",
    "\n",
    "# LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(20, activation='tanh', return_sequences=False, input_shape=(window_size, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X_train, y_train, epochs=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc669807",
   "metadata": {},
   "source": [
    "### 5. Insightful Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10c15d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_test_inv = scaler.inverse_transform(y_test)\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "plt.plot(y_test_inv, label='True')\n",
    "plt.plot(y_pred_inv, label='Predicted')\n",
    "plt.title(\"LSTM Forecast on Time Series\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986da396",
   "metadata": {},
   "source": [
    "Overlaying predictions and true values helps evaluate sequential tracking and drift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049e6e5e",
   "metadata": {},
   "source": [
    "### 6. Algorithm Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e02f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_test_inv, y_pred_inv)\n",
    "print(f\"Test MSE: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c26507",
   "metadata": {},
   "source": [
    "### 7. Pros, Cons, and Techniques\n",
    "\n",
    "**Strengths:**\n",
    "- Captures sequential dependencies\n",
    "- Handles variable-length input\n",
    "- Works well with noisy signals\n",
    "\n",
    "**Limitations:**\n",
    "- Harder to train and tune\n",
    "- Requires more data\n",
    "- Interpretability is limited\n",
    "\n",
    "**Enhancements**:\n",
    "- Use GRUs for lighter models\n",
    "- Add dropout to mitigate overfitting\n",
    "- Use bidirectional RNNs for contextual understanding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1e9d13",
   "metadata": {},
   "source": [
    "### 8. Further Explorations\n",
    "\n",
    "- TODO: Compare LSTM and GRU performance\n",
    "- TODO: Try bidirectional or stacked RNNs\n",
    "- TODO: Test on real-world time series datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1cb17c",
   "metadata": {},
   "source": [
    "### 9. Summary & Resources\n",
    "\n",
    "**Key Insights:**\n",
    "- RNNs handle time-based and sequential data through memory units.\n",
    "- LSTMs and GRUs solve the vanishing gradient problem.\n",
    "- Sequential learning allows modeling of dependencies over time.\n",
    "\n",
    "**Further Reading:**\n",
    "- “Sequence Modeling” – Chollet, Deep Learning with Python\n",
    "- Keras Documentation: LSTM, GRU layers\n",
    "- Hochreiter & Schmidhuber (1997) - Long Short-Term Memory\n",
    "\n",
    "**Notebook Repo**: (add your GitHub link)  \n",
    "**Companion Article**: (add Medium/Substack link)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
